
@misc{liu_kan_2025,
  title      = {{KAN}: Kolmogorov-Arnold Networks},
  url        = {http://arxiv.org/abs/2404.19756},
  doi        = {10.48550/arXiv.2404.19756},
  shorttitle = {{KAN}},
  abstract   = {Inspired by the Kolmogorov-Arnold representation theorem, we propose Kolmogorov-Arnold Networks ({KANs}) as promising alternatives to Multi-Layer Perceptrons ({MLPs}). While {MLPs} have fixed activation functions on nodes ("neurons"), {KANs} have learnable activation functions on edges ("weights"). {KANs} have no linear weights at all -- every weight parameter is replaced by a univariate function parametrized as a spline. We show that this seemingly simple change makes {KANs} outperform {MLPs} in terms of accuracy and interpretability. For accuracy, much smaller {KANs} can achieve comparable or better accuracy than much larger {MLPs} in data fitting and {PDE} solving. Theoretically and empirically, {KANs} possess faster neural scaling laws than {MLPs}. For interpretability, {KANs} can be intuitively visualized and can easily interact with human users. Through two examples in mathematics and physics, {KANs} are shown to be useful collaborators helping scientists (re)discover mathematical and physical laws. In summary, {KANs} are promising alternatives for {MLPs}, opening opportunities for further improving today's deep learning models which rely heavily on {MLPs}.},
  number     = {{arXiv}:2404.19756},
  publisher  = {{arXiv}},
  author     = {Liu, Ziming and Wang, Yixuan and Vaidya, Sachin and Ruehle, Fabian and Halverson, James and Soljačić, Marin and Hou, Thomas Y. and Tegmark, Max},
  urldate    = {2025-12-21},
  date       = {2025-02-09},
  eprinttype = {arxiv},
  eprint     = {2404.19756 [cs]},
  keywords   = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks, Statistics - Machine Learning},
  file       = {Preprint PDF:/home/schlafel/Zotero/storage/AWCSLZZJ/Liu et al. - 2025 - KAN Kolmogorov-Arnold Networks.pdf:application/pdf;Snapshot:/home/schlafel/Zotero/storage/STE2XJHC/2404.html:text/html}
}

@video{serranoacademy_kolmogorov-arnold_2024,
  title    = {Kolmogorov-Arnold Networks ({KANs}) - What are they and how do they work?},
  url      = {https://www.youtube.com/watch?v=myFtp5zMv8U},
  abstract = {Kolmogorov-Arnold networks are a beautiful and high-performing architecture for neural networks, based on the Kolmogorov-Arnold representation theorem. Learn how they work in this friendly video!
              Original paper: https://arxiv.org/abs/2404.19756},
  author   = {{Serrano.Academy}},
  urldate  = {2025-12-21},
  date     = {2024-03-12}
}

@online{nadis_novel_2024,
  title      = {Novel Architecture Makes Neural Networks More Understandable},
  url        = {https://www.quantamagazine.org/novel-architecture-makes-neural-networks-more-understandable-20240911/},
  abstract   = {By tapping into a decades-old mathematical principle, researchers are hoping that Kolmogorov-Arnold networks will facilitate scientific discovery.},
  titleaddon = {Quanta Magazine},
  author     = {Nadis, Steve},
  urldate    = {2026-01-05},
  date       = {2024-09-11},
  langid     = {english},
  file       = {Snapshot:/home/schlafel/Zotero/storage/QTKXC4E4/novel-architecture-makes-neural-networks-more-understandable-20240911.html:text/html}
}

@video{valence_labs_kan_2024,
  title      = {{KAN}: Kolmogorov-Arnold Networks {\textbar} Ziming Liu},
  url        = {https://www.youtube.com/watch?v=AUDHb-tnlB0},
  shorttitle = {{KAN}},
  abstract   = {Portal is the home of the {AI} for drug discovery community. Join for more details on this talk and to connect with the speakers: https://portal.valencelabs.com/logg},
  author     = {{Valence Labs}},
  urldate    = {2026-01-05},
  date       = {2024-05-13}
}

@video{graphics_in_5_minutes_splines_2022,
  title      = {Splines in 5 minutes:  Part 3 -- B-splines and 2D},
  url        = {https://www.youtube.com/watch?v=JwN43QAlF50},
  shorttitle = {Splines in 5 minutes},
  abstract   = {Equivalent to a 50 minute university lecture on B-splines and 2D splines.  Part 3 of 3 on splines.
                Part 1:      • Splines in 5 minutes:  Part 1 -- cubic curves},
  author     = {{Graphics in 5 Minutes}},
  urldate    = {2026-01-05},
  date       = {2022-03-06}
}

@misc{poggio_theoretical_2019,
  title      = {Theoretical Issues in Deep Networks: Approximation, Optimization and Generalization},
  url        = {http://arxiv.org/abs/1908.09375},
  doi        = {10.48550/arXiv.1908.09375},
  shorttitle = {Theoretical Issues in Deep Networks},
  abstract   = {While deep learning is successful in a number of applications, it is not yet well understood theoretically. A satisfactory theoretical characterization of deep learning however, is beginning to emerge. It covers the following questions: 1) representation power of deep networks 2) optimization of the empirical risk 3) generalization properties of gradient descent techniques --- why the expected error does not suffer, despite the absence of explicit regularization, when the networks are overparametrized? In this review we discuss recent advances in the three areas. In approximation theory both shallow and deep networks have been shown to approximate any continuous functions on a bounded domain at the expense of an exponential number of parameters (exponential in the dimensionality of the function). However, for a subset of compositional functions, deep networks of the convolutional type can have a linear dependence on dimensionality, unlike shallow networks. In optimization we discuss the loss landscape for the exponential loss function and show that stochastic gradient descent will find with high probability the global minima. To address the question of generalization for classification tasks, we use classical uniform convergence results to justify minimizing a surrogate exponential-type loss function under a unit norm constraint on the weight matrix at each layer -- since the interesting variables for classification are the weight directions rather than the weights. Our approach, which is supported by several independent new results, offers a solution to the puzzle about generalization performance of deep overparametrized {ReLU} networks, uncovering the origin of the underlying hidden complexity control.},
  number     = {{arXiv}:1908.09375},
  publisher  = {{arXiv}},
  author     = {Poggio, Tomaso and Banburski, Andrzej and Liao, Qianli},
  urldate    = {2026-01-10},
  date       = {2019-08-25},
  eprinttype = {arxiv},
  eprint     = {1908.09375 [cs]},
  keywords   = {Computer Science - Machine Learning, Statistics - Machine Learning},
  file       = {Preprint PDF:/home/schlafel/Zotero/storage/IS3GXR7H/Poggio et al. - 2019 - Theoretical Issues in Deep Networks Approximation, Optimization and Generalization.pdf:application/pdf;Snapshot:/home/schlafel/Zotero/storage/RFZ76LST/1908.html:text/html}
}

@article{girosi_representation_1989,
  title        = {Representation Properties of Networks: Kolmogorov's Theorem Is Irrelevant},
  volume       = {1},
  issn         = {0899-7667},
  url          = {https://doi.org/10.1162/neco.1989.1.4.465},
  doi          = {10.1162/neco.1989.1.4.465},
  shorttitle   = {Representation Properties of Networks},
  abstract     = {Many neural networks can be regarded as attempting to approximate a multivariate function in terms of one-input one-output units. This note considers the problem of an exact representation of nonlinear mappings in terms of simpler functions of fewer variables. We review Kolmogorov's theorem on the representation of functions of several variables in terms of functions of one variable and show that it is irrelevant in the context of networks for learning.},
  pages        = {465--469},
  number       = {4},
  journaltitle = {Neural Computation},
  shortjournal = {Neural Comput},
  author       = {Girosi, Federico and Poggio, Tomaso},
  urldate      = {2026-01-10},
  date         = {1989-12-01},
  file         = {Snapshot:/home/schlafel/Zotero/storage/7FCLCYD3/Representation-Properties-of-Networks-Kolmogorov-s.html:text/html}
}

@misc{yu2024kanmlpfairercomparison,
  title      = {{KAN} or {MLP}: {A} {Fairer} {Comparison}},
  shorttitle = {{KAN} or {MLP}},
  url        = {http://arxiv.org/abs/2407.16674},
  doi        = {10.48550/arXiv.2407.16674},
  abstract   = {This paper does not introduce a novel method. Instead, it offers a fairer and more comprehensive comparison of KAN and MLP models across various tasks, including machine learning, computer vision, audio processing, natural language processing, and symbolic formula representation. Specifically, we control the number of parameters and FLOPs to compare the performance of KAN and MLP. Our main observation is that, except for symbolic formula representation tasks, MLP generally outperforms KAN. We also conduct ablation studies on KAN and find that its advantage in symbolic formula representation mainly stems from its B-spline activation function. When B-spline is applied to MLP, performance in symbolic formula representation significantly improves, surpassing or matching that of KAN. However, in other tasks where MLP already excels over KAN, B-spline does not substantially enhance MLP's performance. Furthermore, we find that KAN's forgetting issue is more severe than that of MLP in a standard class-incremental continual learning setting, which differs from the findings reported in the KAN paper. We hope these results provide insights for future research on KAN and other MLP alternatives. Project link: https://github.com/yu-rp/KANbeFair},
  urldate    = {2026-01-12},
  publisher  = {arXiv},
  author     = {Yu, Runpeng and Yu, Weihao and Wang, Xinchao},
  month      = aug,
  year       = {2024},
  note       = {arXiv:2407.16674 [cs]},
  keywords   = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
  annote     = {Comment: Technical Report},
  file       = {Preprint PDF:/home/felsch01/Zotero/storage/LI5ZL7YR/Yu et al. - 2024 - KAN or MLP A Fairer Comparison.pdf:application/pdf;Snapshot:/home/felsch01/Zotero/storage/MHAL46GU/2407.html:text/html}
}

@misc{li2024kolmogorovarnoldnetworksradialbasis,
  title     = {Kolmogorov-{Arnold} {Networks} are {Radial} {Basis} {Function} {Networks}},
  url       = {http://arxiv.org/abs/2405.06721},
  doi       = {10.48550/arXiv.2405.06721},
  abstract  = {This short paper is a fast proof-of-concept that the 3-order B-splines used in Kolmogorov-Arnold Networks (KANs) can be well approximated by Gaussian radial basis functions. Doing so leads to FastKAN, a much faster implementation of KAN which is also a radial basis function (RBF) network.},
  urldate   = {2026-01-12},
  publisher = {arXiv},
  author    = {Li, Ziyao},
  month     = may,
  year      = {2024},
  note      = {arXiv:2405.06721 [cs]},
  keywords  = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
  file      = {Preprint PDF:/home/felsch01/Zotero/storage/EU4DTU52/Li - 2024 - Kolmogorov-Arnold Networks are Radial Basis Function Networks.pdf:application/pdf;Snapshot:/home/felsch01/Zotero/storage/AD5ZB8EP/2405.html:text/html}
}

